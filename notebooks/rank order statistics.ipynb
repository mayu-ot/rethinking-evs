{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute rank order statistics on human annotated importance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kendalltau, spearmanr\n",
    "from scipy.stats import rankdata\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/')\n",
    "from tools.io import load_tvsum_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_tvsum_mat('./data/raw/tvsum/ydata-tvsum50.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rc_func(metric):\n",
    "    if metric == 'kendalltau':\n",
    "        f = lambda x, y: kendalltau(rankdata(-x), rankdata(-y))\n",
    "    elif metric == 'spearmanr':\n",
    "        f = lambda x, y: spearmanr(x, y)\n",
    "    else:\n",
    "        raise RuntimeError\n",
    "    return f\n",
    "\n",
    "class RankCorrelationEvaluator(object):\n",
    "    \n",
    "    def __call__(self):\n",
    "        res = []\n",
    "        for d in data:\n",
    "            user_anno = d['user_anno'].T\n",
    "            N = user_anno.shape[1]\n",
    "\n",
    "            D = []\n",
    "            mean_tau = []\n",
    "            min_tau = []\n",
    "            max_tau = []\n",
    "\n",
    "            pred_x = self.get_score(d['video'])\n",
    "            D = [self.rc_func(x, pred_x)[0] for x in user_anno]\n",
    "\n",
    "            res.append({'video': d['video'],\n",
    "                        'mean': np.mean(D),\n",
    "                       'min': np.min(D), \n",
    "                       'max': np.max(D), \n",
    "                        'cc': np.asarray(D)\n",
    "                       })\n",
    "        return res\n",
    "\n",
    "class HumanEvaluator(RankCorrelationEvaluator):\n",
    "    def __init__(self, metric):\n",
    "        self.rc_func = get_rc_func(metric)\n",
    "    \n",
    "    def __call__(self):\n",
    "        res = []\n",
    "        for d in data:\n",
    "            user_anno = d['user_anno'].T\n",
    "            N = user_anno.shape[1]\n",
    "            \n",
    "            max_rc = []\n",
    "            min_rc = []\n",
    "            avr_rc = []\n",
    "            rc = []\n",
    "            for i, x in enumerate(user_anno):\n",
    "                R = [self.rc_func(x, user_anno[j])[0] for j in range(len(user_anno)) if j != i]\n",
    "                \n",
    "                max_rc.append(max(R))\n",
    "                min_rc.append(min(R))\n",
    "                avr_rc.append(np.mean(R))\n",
    "                rc += R\n",
    "                \n",
    "            res.append({'video': d['video'],\n",
    "                        'mean': np.mean(avr_rc),\n",
    "                       'min': np.mean(min_rc), \n",
    "                       'max': np.mean(max_rc), \n",
    "                        'cc': np.asarray(rc)\n",
    "                       })\n",
    "        return res\n",
    "    \n",
    "class RandomEvaluator(RankCorrelationEvaluator):\n",
    "    def __init__(self, metric):\n",
    "        self.rc_func = get_rc_func(metric)\n",
    "        \n",
    "        rand_scores = {}\n",
    "        for d in data:\n",
    "            user_anno = d['user_anno'].T\n",
    "            N = user_anno.shape[1]\n",
    "            rand_scores[d['video']] = np.random.random((N,))\n",
    "            \n",
    "        self.rand_scores = rand_scores\n",
    "            \n",
    "    def get_score(self, v_id):\n",
    "        return self.rand_scores[v_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human: mean 0.204\n"
     ]
    }
   ],
   "source": [
    "metric = 'spearmanr'\n",
    "human_res = HumanEvaluator(metric)()\n",
    "mean_arr = np.asarray([x['mean'] for x in human_res])\n",
    "print('human'+': mean %.3f'%(np.mean(mean_arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human: mean 0.177\n"
     ]
    }
   ],
   "source": [
    "metric = 'kendalltau'\n",
    "human_res = HumanEvaluator(metric)()\n",
    "mean_arr = np.asarray([x['mean'] for x in human_res])\n",
    "print('human'+': mean %.3f'%(np.mean(mean_arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
